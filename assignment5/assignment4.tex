\documentclass[paper=a4,fontsize=10pt,DIV11,BCOR10mm]{scrartcl}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{graphicx}

\usepackage{url}
\usepackage{eurosym}


\usepackage{url}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\tr}{tr}




\titlehead{Technische Universität Berlin -- Fachgebiet Maschinelles Lernen\hfill \parbox[t]{2cm}{\includegraphics[width=2cm]{../TU_Logo_kurz_RGB_rot}}}

\begin{document}

\title{Maschinelles Lernen 1 - Assignment 4\\
\small{Technische Universität Berlin}}


\author{\small{Christoph Conrads (315565)}\and \small{Antje Relitz (327289)}  \and \small{Benjamin Pietrowicz (332542)} \and \small{Mitja Richter (324680)} }

\date{WS 2013/2014}

\maketitle


\section*{Probabilistic PCA}
\subsection*{a)}
We want to show that $x \sim \mathcal{N}(\mu,C)$. In the proof we use the fact that the characteristic function of a multivariate Gaussian distribution $y \sim \mathcal{N}(\mu_0,\Sigma_0)$ is given by 
\begin{equation*}
\phi_y(t)=e^{-\frac{1}{2}i t^T \mu_0 + t^T \Sigma_0 t}
\end{equation*}
as well as the fact that the characteristic function of a sum of random variables is the product of characteristic functions of those random variables.\\

One more precondition that we will need is the fact that if $y \sim \mathcal{N}(\mu_0,\Sigma_0)$, then for a matrix $W$, the distribution of $Wy$ is given by $Wy \sim \mathcal{N}(W\mu_0,W\Sigma_0W^T)$.\\

To simplify notation, we will write, that $\mu$ is Gaussian distributed with mean $\mu$ and covariance matrix \underline{\underline{$0$}}. (This would theoretically cause problems, because the covariance matrix is supposed to be invertible, but we could also set the covariance matrix to be $\lambda I$ and then look at the limit $\lim\limits_{\lambda \rightarrow 0}\lambda I$, which would not change the result).\\

Therefore the individual characteristic functions are given by
\begin{align*}
\phi_{Wh}(t) &= \exp\{{-\frac{1}{2}i t^T W\underline{0} + t^T W I W^T t}\}\\
\phi_\mu(t) &= \exp\{{-\frac{1}{2}i t^T \underline{0} + t^T \underline{\underline{0}} t}\}\\
\phi_\epsilon(t) &= \exp \{{-\frac{1}{2}i t^T \underline{0} + t^T \sigma^2 I t}\}\text{.}\\
\end{align*}

With this preliminaries we do now obtain:
\begin{align*}
\phi_x(t) &= \phi_{Wh}(t) \cdot \phi_\mu(t) \cdot \phi_\epsilon(t)\\
&= \exp\{{-\frac{1}{2}i t^T W\underline{0} + t^T W I W^T t}\} \cdot \exp\{{-\frac{1}{2}i t^T \mu + t^T \underline{\underline{0}} t}\} \cdot \exp \{{-\frac{1}{2}i t^T \underline{0} + t^T \sigma^2 I t}\}\\
&=\exp\{{-\frac{1}{2}i t^T \mu + t^T W W^T t + t^T \sigma^2 I t}\}\\
&=\exp\{{-\frac{1}{2}i t^T \mu + t^T( W W^T +\sigma^2 I)t}\}\text{.}\\
\end{align*}
Therefore $x$ is multivariate Gaussian distributed with mean $\mu$ and covariance matrix $C=W W^T +\sigma^2 I$.

\subsection*{b)}
The covariance matrix is given by $C=W W^T +\sigma^2 I$. It can easily be seen that $C$ is symmetric and positive semi-definite (for any choice of $\sigma$). As $\sigma$ increases, the matrix $C$ becomes diagonally dominant and the diagonal entries become positive (since we add positive terms to the diagonal of $C$ only). We can now apply a result from linear algebra, that tells us that a symmetric diagonally dominant matrix with positive diagonal is also positive definite. So if the parameter $\sigma$ is large enough, the covariance matrix $C$ is positive definite and therefore invertible.\\

Since $WW^T$ is symmetric, there exist $d$ distinct eigenvectors $v_1,\dots, v_d$ of $WW^T$ corresponding to the eigenvalues $\mu_1,\dots \mu_d$ of $WW^T$ respectively, where the eigenvalues are ordered, such that $\mu_1 \geq \mu_2 \geq \dots \geq \mu_d$. 
Since 
\begin{equation*}
Cv_i=(WW^Tv_i+\sigma^2 I v_i)= \mu_i v_i +\sigma^2 v_i = (\mu_i+\sigma^2)v_i
\end{equation*}
holds, every eigenvector of $WW^T$ is also an eigenvector of $C$. Since $C$ is of dimension $d \times d$, $C$ can not have more than $d$ eigenvectors and therefore each eigenvector of $C$ is also an eigenvector of $WW^T$. We see that $\sigma^2$ is an additive constant, that is added to each eigenvalue $\mu_i$ and therefore the eigenvector $v_k$ corresponding to $\mu_k+\sigma^2$ ($k$-th principal component of $C$) is the same as the eigenvector of $WW^T$ corresponding to $\mu_k$, which is the $k$-th principal component of $WW^T$. Therefore the parameter $\sigma^2$ does not change the ordering of the principal components.

\subsection*{c)}
The complexity of $WW^T$ (and therefore the complexity of $C$ as well) increases with increasing $q$. This is due to the fact that we can interprete $WW^T$ as a multiple of the covariance matrix of measured datapoints $[w_1,\dots, w_q]=W$. As $q$ increases, the number of data points increases and the complexity of the covariance matrix $WW^T$ increases.\\

The idea behind probabilistic PCA is that the $d$-dimensional observed data is actually linearly dependent on some $q$-dimensional random distribution (and shifted by $\mu$ and falsified by some noise). Therefore it does not make sense to choose $q$ bigger or equal than $d$, since then we would try to explain our $d$-dimensional observation by a multivariate random distribution of dimension $d$ or more. This would lead to overfitting and therefore $q$ should be chosen to be smaller than $d$.
 
%\begin{thebibliograpy}
%	\bibitem{DHS2000} R. O. Duda, P. E. Hart and D. G. Stork. \emph{Pattern Classification}. 2nd ed. New York, NY, USA: Wiley-Interscience, 2000. ISBN: 0-4710-5669-3
%\end{thebibliograpy}

\end{document}
