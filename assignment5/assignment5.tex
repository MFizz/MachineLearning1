\documentclass[paper=a4,fontsize=10pt,DIV11,BCOR10mm]{scrartcl}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{graphicx}

\usepackage{url}
\usepackage{eurosym}


\usepackage{url}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\tr}{tr}




\titlehead{Technische Universität Berlin -- Fachgebiet Maschinelles Lernen\hfill \parbox[t]{2cm}{\includegraphics[width=2cm]{../TU_Logo_kurz_RGB_rot}}}

\begin{document}

\title{Maschinelles Lernen 1 - Assignment 5\\
\small{Technische Universität Berlin}}


\author{\small{Christoph Conrads (315565)}\and \small{Antje Relitz (327289)}  \and \small{Benjamin Pietrowicz (332542)} \and \small{Mitja Richter (324680)} }

\date{WS 2013/2014}

\maketitle


\section{Probabilistic PCA}
\subsection*{a)}
We want to show that $x \sim \mathcal{N}(\mu,C)$. In the proof we use the fact that the characteristic function of a multivariate Gaussian distribution $y \sim \mathcal{N}(\mu_0,\Sigma_0)$ is given by 
\begin{equation*}
\phi_y(t)=e^{-\frac{1}{2}i t^T \mu_0 + t^T \Sigma_0 t}
\end{equation*}
as well as the fact that the characteristic function of a sum of random variables is the product of characteristic functions of those random variables.\\

One more precondition that we will need is the fact that if $y \sim \mathcal{N}(\mu_0,\Sigma_0)$, then for a matrix $W$, the distribution of $Wy$ is given by $Wy \sim \mathcal{N}(W\mu_0,W\Sigma_0W^T)$.\\

To simplify notation, we will write, that $\mu$ is Gaussian distributed with mean $\mu$ and covariance matrix \underline{\underline{$0$}}. (This would theoretically cause problems, because the covariance matrix is supposed to be invertible, but we could also set the covariance matrix to be $\lambda I$ and then look at the limit $\lim\limits_{\lambda \rightarrow 0}\lambda I$, which would not change the result).\\

Therefore the individual characteristic functions are given by
\begin{align*}
\phi_{Wh}(t) &= \exp\{{-\frac{1}{2}i t^T W\underline{0} + t^T W I W^T t}\}\\
\phi_\mu(t) &= \exp\{{-\frac{1}{2}i t^T \underline{0} + t^T \underline{\underline{0}} t}\}\\
\phi_\epsilon(t) &= \exp \{{-\frac{1}{2}i t^T \underline{0} + t^T \sigma^2 I t}\}\text{.}\\
\end{align*}

With this preliminaries we do now obtain:
\begin{align*}
\phi_x(t) &= \phi_{Wh}(t) \cdot \phi_\mu(t) \cdot \phi_\epsilon(t)\\
&= \exp\{{-\frac{1}{2}i t^T W\underline{0} + t^T W I W^T t}\} \cdot \exp\{{-\frac{1}{2}i t^T \mu + t^T \underline{\underline{0}} t}\} \cdot \exp \{{-\frac{1}{2}i t^T \underline{0} + t^T \sigma^2 I t}\}\\
&=\exp\{{-\frac{1}{2}i t^T \mu + t^T W W^T t + t^T \sigma^2 I t}\}\\
&=\exp\{{-\frac{1}{2}i t^T \mu + t^T( W W^T +\sigma^2 I)t}\}\text{.}\\
\end{align*}
Therefore $x$ is multivariate Gaussian distributed with mean $\mu$ and covariance matrix $C=W W^T +\sigma^2 I$.

\subsection*{b)}
The covariance matrix is given by $C=W W^T +\sigma^2 I$. It can easily be seen that $C$ is symmetric and positive semi-definite (for any choice of $\sigma$). As $\sigma$ increases, the matrix $C$ becomes diagonally dominant and the diagonal entries become positive (since we add positive terms to the diagonal of $C$ only). We can now apply a result from linear algebra, that tells us that a symmetric diagonally dominant matrix with positive diagonal is also positive definite. So if the parameter $\sigma$ is large enough, the covariance matrix $C$ is positive definite and therefore invertible.\\

Since $WW^T$ is symmetric, there exist $d$ distinct eigenvectors $v_1,\dots, v_d$ of $WW^T$ corresponding to the eigenvalues $\mu_1,\dots \mu_d$ of $WW^T$ respectively, where the eigenvalues are ordered, such that $\mu_1 \geq \mu_2 \geq \dots \geq \mu_d$. 
Since 
\begin{equation*}
Cv_i=(WW^Tv_i+\sigma^2 I v_i)= \mu_i v_i +\sigma^2 v_i = (\mu_i+\sigma^2)v_i
\end{equation*}
holds, every eigenvector of $WW^T$ is also an eigenvector of $C$. Since $C$ is of dimension $d \times d$, $C$ can not have more than $d$ eigenvectors and therefore each eigenvector of $C$ is also an eigenvector of $WW^T$. We see that $\sigma^2$ is an additive constant, that is added to each eigenvalue $\mu_i$ and therefore the eigenvector $v_k$ corresponding to $\mu_k+\sigma^2$ ($k$-th principal component of $C$) is the same as the eigenvector of $WW^T$ corresponding to $\mu_k$, which is the $k$-th principal component of $WW^T$. Therefore the parameter $\sigma^2$ does not change the ordering of the principal components.

\subsection*{c)}
The complexity of $WW^T$ (and therefore the complexity of $C$ as well) increases with increasing $q$. This is due to the fact that we can interprete $WW^T$ as a multiple of the covariance matrix of measured datapoints $[w_1,\dots, w_q]=W$. As $q$ increases, the number of data points increases and the complexity of the covariance matrix $WW^T$ increases.\\

The idea behind probabilistic PCA is that the $d$-dimensional observed data is actually linearly dependent on some $q$-dimensional random distribution (and shifted by $\mu$ and falsified by some noise). Therefore it does not make sense to choose $q$ bigger or equal than $d$, since then we would try to explain our $d$-dimensional observation by a multivariate random distribution of dimension $d$ or more. This would lead to overfitting and therefore $q$ should be chosen to be smaller than $d$.




\section{Generalized Rayleigh Quotient}

\subsection*{a)}

Let $S_W, S_B \in \mathbb{R}^{d,d}$ where $S_W$ is symmetric positive definite. Let $A \in \mathbb{R}^{d,d}$ so that $A^T A = S_W$. Then
\[ J(w) = \frac{w^T S_B w}{w^T S_W w} = \frac{w^T S_B w}{(w^T A^T) (A w)} = \frac{w^T S_B w}{(A w)^T (A w)} \]
Let $v := A w$, then
\[ \bar{J}(v) = \frac{w^T A^T A^{-T} S_B A^{-1} A w}{v^T v} = \frac{v^T A^{-T} S_B A^{-1} v}{v^T v} = \frac{v^T M v}{v^T v} \]
where $M := A^{-T} S_B A^{-1}$.



\subsection*{b)}

We assume that there is a maximum. Let $w \neq 0$ be a maximizer of $\bar{J}$ and let $v = w / \norm{w} \Rightarrow \norm{v} = 1$. Thus
\[ \max_u \bar{J}(u) = \bar{J}(w) = \frac{w^T M w}{w^T w} = \frac{\norm{w}^2 v^T M v}{\norm{w}^2 v^T v} = \frac{v^T M v}{v^T v} = \bar{J}(v) \]
Therefore $v$ is a maximizer as well.



\subsection*{c)}

Using b) we look for vectors $v$ that maximize $\bar{J}$ subject to $\norm{v}_2 = 1$. Because the scaling of the vector has no influence on the function value $\bar{J}$ we choose $f(x) = x^T M x$ and $g(x) = x^T x - 1$. We are looking for
\[ \max_v f(v) - \lambda g(v) \text{ subject to } g(v) = 0 \]
The derivatives give
\begin{flalign*}
	&\frac{\partial f}{\partial v} = 2 M v \\
	&\frac{\partial g}{\partial v} = 2 v \\
\end{flalign*}
A necessary condition for the maximum is therefore
\[ 2 M v - 2 \lambda v = 0 \Leftrightarrow M v = \lambda v \]
which is maximal for the largest eigenvalue of $M$.



\subsection*{d)}

Let $v$ be an eigenvector corresponding to an eigenvalue $\lambda$ of $S_W^{-1} S_B$. Then
\[ J(v) = \frac{v^T S_B v}{v^T S_W v} = \frac{v^T S_B v}{v^T S_W (S_W^{-1} S_B v / \lambda)} = \lambda \frac{v^T S_B v}{v^T S_B v} = \lambda \]
i.\,e., if $\lambda$ is the largest eigenvalue then $J$ is maximal.

 
%\begin{thebibliograpy}
%	\bibitem{DHS2000} R. O. Duda, P. E. Hart and D. G. Stork. \emph{Pattern Classification}. 2nd ed. New York, NY, USA: Wiley-Interscience, 2000. ISBN: 0-4710-5669-3
%\end{thebibliograpy}

\end{document}
