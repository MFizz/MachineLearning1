\documentclass[paper=a4,fontsize=10pt,DIV11,BCOR10mm]{scrartcl}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{graphicx}

\usepackage{url}
\usepackage{eurosym}


\usepackage{url}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\tr}{tr}




\titlehead{Technische Universität Berlin -- Fachgebiet Maschinelles Lernen\hfill \parbox[t]{2cm}{\includegraphics[width=2cm]{../TU_Logo_kurz_RGB_rot}}}

\begin{document}

\title{Maschinelles Lernen 1 - Assignment 3\\
\small{Technische Universität Berlin}}


\author{\small{Christoph Conrads (315565)}\and \small{Antje Relitz (327289)}  \and \small{Benjamin Pietrowicz (332542)} \and \small{Mitja Richter (324680)} }

\date{WS 2013/2014}

\maketitle


\section{Matrix Calculus}

\subsection*{a)}

\subsection*{b)}

\subsection*{c)}





\section{Lagrange Multipliers}

\subsection*{a)}

Wir suchen einen Punkt $\theta^*$, der so nahe wie möglich an $m$ ist und $b^T \theta^* < 0$ erfüllt. Falls die Nebenbedingung von $m$ erfüllt ist, wählen wir $\theta^* = m$. Ansonsten erhalten wir diesen Punkt, indem wir in Richtung $-b$ laufen, weil $b$ der Normalenvektor der Hyperebene ist, die zulässige und unzulässige Lösungen voneinander trennt. Dann ist $\theta^* = m - \lambda^* b$, wobei $\lambda^*$ angibt wie weit wir uns von $m$ in Richtung $-b$ bewegen müssen. Es gilt
\[ b^T \theta = b^T (m - \lambda b) = b^T m - \lambda b^T < 0 \Leftrightarrow \lambda > \frac{b^T m}{b^T b} \]
Dann wählen wir uns ein minimales $\lambda^*$, das obige Bedingung erfüllt, und erhalten damit $\theta^*$.



\subsection*{b)}

Wir suchen wieder einen Punkt $\theta^*$, der so nahe wie möglich an $m$ liegt und die gegebene Bedingung erfüllt. Falls $\norm{\mu - m}_2 < 1$ gilt, wählen wir $\theta^* = m$. Ansonsten suchen wir einen Punkt, der sich auf der Geraden $\overrightarrow{m \mu}$ befindet. Somit ist $\theta^* = \mu + \lambda^* (m - \mu)$. Die Nebenbedingung ergibt dann
\[ \norm{\lambda (m - \mu)}_2 = \abs{\lambda} \norm{m - \mu}_2 < 1 \Leftrightarrow \abs{\lambda} < \frac{1}{\norm{m - \mu}_2} \]
Wir wollen soweit wie möglich in Richtung von $m$ gehen und wählen $\lambda^* = \max \lambda$.



\subsection*{c)}

$\theta^T b < 0$ restricts the solution to one side of the given hyperplane $b$ whereas $\norm{\theta - \mu}_2^2 < 1$ restricts the solution to the inside of a sphere with center $\mu$ and radius 1.





\section{Principal Component Analysis}

\subsection*{a)}

Es gilt $S_{ii} > 0 \forall i$ und $\lambda_i > 0 \forall i$, weil $S = X X^T$ und $S$ damit positiv definit ist. Außerdem gilt
\[ \tr S = \sum_i \lambda_i = \lambda_1 + \underbrace{\sum_{i > 1} \lambda_i}_{\geq 0} \]
und damit ist $\tr S$ eine obere Schranke für $\tr S$.



\subsection*{b)}

$S$ hat maximal einen von null verschiedenen Eigenwert.



\subsection*{c)}

OBdA sei $V_1 = \max \{ V_1, \cdots, V_d \}$ und sei $\lambda_1 < V_1$. Sei $e_1 = [1, 0, \cdots, 0]^T \in \mathbb{R}^d$. Dann ist $e_1^T S e_1$ größer als der maximale Eigenwert. Das ist ein Widerspruch, weil $S$ symmetrisch positiv definit ist.



\subsection*{d)}

$S$ ist diagonal.



%\begin{thebibliograpy}
%	\bibitem{DHS2000} R. O. Duda, P. E. Hart and D. G. Stork. \emph{Pattern Classification}. 2nd ed. New York, NY, USA: Wiley-Interscience, 2000. ISBN: 0-4710-5669-3
%\end{thebibliograpy}

\end{document}
