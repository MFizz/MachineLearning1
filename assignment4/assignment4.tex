\documentclass[paper=a4,fontsize=10pt,DIV11,BCOR10mm]{scrartcl}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{graphicx}

\usepackage{url}
\usepackage{eurosym}


\usepackage{url}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\tr}{tr}




\titlehead{Technische Universität Berlin -- Fachgebiet Maschinelles Lernen\hfill \parbox[t]{2cm}{\includegraphics[width=2cm]{../TU_Logo_kurz_RGB_rot}}}

\begin{document}

\title{Maschinelles Lernen 1 - Assignment 4\\
\small{Technische Universität Berlin}}


\author{\small{Christoph Conrads (315565)}\and \small{Antje Relitz (327289)}  \and \small{Benjamin Pietrowicz (332542)} \and \small{Mitja Richter (324680)} }

\date{WS 2013/2014}

\maketitle


\section{Matrix Calculus}
The annotations are the equation numbers in the matrix cookbook.

\subsection*{a)}
\begin{align*}
\frac{\partial^2}{\partial x \partial x^T}(x+a)^T \mathbf{W}(x+b) &= \frac{\partial^2}{\partial x \partial x^T} ((x+a)^T \mathbf{W}x + (x+a)^T \mathbf{W}b)\\
&= \frac{\partial^2}{\partial x \partial x^T} (x^T\mathbf{W}x + a^T\mathbf{W}x + x^T\mathbf{W}b + a^T\mathbf{W}b) \stackrel{(90)}{=} \mathbf{W} + \mathbf{W}^T 
\end{align*}

\subsection*{b)}

\begin{align*}
 \frac{\partial}{\partial\mathbf{W}}e^{(x+a)^T\mathbf{W}(x+b)} &= \frac{\partial}{\partial\mathbf{W}}((x+a)^T\mathbf{W}(x+b)) e^{(x+a)^T\mathbf{W}(x+b)} \\
 &\stackrel{(62)}{=} (x+a)(x+b)^Te^{(x+a)^T\mathbf{W}(x+b)}
\end{align*}

\subsection*{c)}

Since $\tr(A)=\sum_{k=1}^d a_{kk}$, we only have to compute terms of the form $\frac{\partial^2}{\partial x_k\partial x_k}$ for all $k=1,..,d$. Let $k\in\{1,\dots,d\}$ be arbitrary.


\begin{align*}
\frac{\partial^2}{\partial x_k\partial x_k} \sum_{p=2}^P \beta_p \norm{x}_p^p &=\frac{\partial^2}{\partial x_k\partial x_k}\sum_{p=2}^P \beta_p \sum_{i=1}^d \lvert x_i \rvert^p =\frac{\partial}{\partial x_k}\sum_{p=2}^P \beta_p \cdot p \cdot \lvert x_k\rvert^{p-1} \frac{x_k}{\lvert x_k\rvert}\\
&=\frac{\partial}{\partial x_k}\sum_{p=2}^P \beta_p \cdot  p \cdot \lvert x_k\rvert^{p-2} x_k \\
&=\sum_{p=2}^P \beta_p \cdot  ( p \cdot (p-2) \cdot \lvert x_k\rvert^{p-3}\frac{x_k}{\lvert x_k\rvert}  x_k +p \cdot \lvert x_k\rvert^{p-2})\\
&=\sum_{p=2}^P \beta_p \cdot ( p \cdot (p-2) \cdot \lvert x_k\rvert^{p-4}\cdot x_k^2 +p \cdot \lvert x_k\rvert^{p-2})\\
&= \sum_{p=2}^P \beta_p \cdot ( p \cdot (p-2) \cdot \lvert x_k\rvert^{p-4}\cdot \lvert x_k \rvert^2 +p \cdot \lvert x_k\rvert^{p-2})\\
&= \sum_{p=2}^P \beta_p \cdot ( p \cdot (p-2) \cdot \lvert x_k\rvert^{p-2}\cdot +p \cdot \lvert x_k\rvert^{p-2})\\
&= \sum_{p=2}^P \beta_p \cdot  p \cdot (p-1) \cdot \lvert x_k\rvert^{p-2}\text{.}
\end{align*}

This differentiation is only valid for the case $x_k \neq 0$.

If we insert this into the equation for the trace we obtain
\begin{equation*}
\tr(\frac{\partial^2}{\partial x\partial x^T} \sum_{p=2}^P \beta_p \norm{x}_p^p ) = \sum\limits_{k=1}^{d}\sum_{p=2}^P \beta_p \cdot  p \cdot (p-1) \cdot \lvert x_k\rvert^{p-2}\text{.}
\end{equation*}


\section{Lagrange Multipliers}

\subsection*{a)}

Wir suchen einen Punkt $\theta^*$, der so nahe wie möglich an $m$ ist und $b^T \theta^* < 0$ erfüllt. Falls die Nebenbedingung von $m$ erfüllt ist, wählen wir $\theta^* = m$. Ansonsten erhalten wir diesen Punkt, indem wir in Richtung $-b$ laufen, weil $b$ der Normalenvektor der Hyperebene ist, die zulässige und unzulässige Lösungen voneinander trennt. Dann ist $\theta^* = m - \lambda^* b$, wobei $\lambda^*$ angibt wie weit wir uns von $m$ in Richtung $-b$ bewegen müssen. Es gilt
\[ b^T \theta = b^T (m - \lambda b) = b^T m - \lambda b^T < 0 \Leftrightarrow \lambda > \frac{b^T m}{b^T b} \]
Dann wählen wir uns ein minimales $\lambda^*$, das obige Bedingung erfüllt, und erhalten damit $\theta^*$.



\subsection*{b)}

Wir suchen wieder einen Punkt $\theta^*$, der so nahe wie möglich an $m$ liegt und die gegebene Bedingung erfüllt. Falls $\norm{\mu - m}_2 < 1$ gilt, wählen wir $\theta^* = m$. Ansonsten suchen wir einen Punkt, der sich auf der Geraden $\overrightarrow{m \mu}$ befindet. Somit ist $\theta^* = \mu + \lambda^* (m - \mu)$. Die Nebenbedingung ergibt dann
\[ \norm{\lambda (m - \mu)}_2 = \abs{\lambda} \norm{m - \mu}_2 < 1 \Leftrightarrow \abs{\lambda} < \frac{1}{\norm{m - \mu}_2} \]
Wir wollen soweit wie möglich in Richtung von $m$ gehen und wählen $\lambda^* = \max \lambda$.



\subsection*{c)}

$\theta^T b < 0$ restricts the solution to one side of the given hyperplane $b$ whereas $\norm{\theta - \mu}_2^2 < 1$ restricts the solution to the inside of a sphere with center $\mu$ and radius 1.





\section{Principal Component Analysis}

\subsection*{a)}

We first show that the matrix $S=XX^T$, where $X=[x_1-m,\dots, x_n-m]$,  is positive semi-definite. Let $v \in \mathbb{R}^d$ be arbitrary. Then it holds that
\begin{equation*}
v^TSv=v^TXX^Tv=\lVert X^Tv\rVert_2^2 \geq 0
\end{equation*}
and therefore $S$ is positive semi-definite and has non-negative eigenvalues only.
We now use the relation $\tr S=\sum_{i=1}^d \lambda_i$, where $\lambda_i$ denotes the $i$th eigenvalue of $S$.
Hence it follows that
\begin{equation}\label{glg1}
\sum_{i=1}^d V_i=\tr S=\sum_{i=1}^d \lambda_i= \lambda_1 + \underbrace{\sum_{i=2}^d \lambda_i}_{\geq 0} \geq \lambda_1\text{.}
\end{equation}
Therefore $\sum_{i=1}^n V_i$ is an upper bound for the largest eigenvalue of $S$.


\subsection*{b)}

The upper bound in equation \ref{glg1} will only be tight in the case, where $\lambda_i=0$ $\forall i \in \{2,\dots,d\}$. This means the upper bound will be tight if and only if the matrix $S$ has at most one non-zero eigenvalue.


\subsection*{c)}

Since $S$ is obviously symmetric, we know that there exists an orthonormal basis $(v_1,\dots,v_d)$ of eigenvectors of $S$ corresponding to the eigenvalues $\lambda_1,\dots,\lambda_d$ of $S$. Therefore each vector $w\in \mathbb{R}^d$ can be written as a linear combination of eigenvectors of $S$, $w=\sum_{i=1}^d \alpha_iv_i$, where $\alpha_i \in \mathbb{R} \text{ for } i \in \{1,\dots d\}$.
Let $w$ have euclidean norm $\lVert w \rVert_2=1$, i.e. $\sum_{i=1}^d \alpha_i^2=1$. Then we obtain
\begin{equation}\label{glg2}
w^TSw=(\sum_{i=1}^d \alpha_iv_i)^TS(\sum_{i=1}^d \alpha_iv_i)=(\sum_{i=1}^d \alpha_iv_i)^T(\sum_{i=1}^d \lambda_i \alpha_i v_i)
=\sum_{i=1}^d \lambda_i\alpha_i^2 \underbrace{v_i^Tv_i}_{=1} \leq \lambda_1 \sum_{i=1}^d\alpha_i^2=\lambda_1\text{.}
\end{equation} 

Now we assume without loss of generality that $V_1=\max\{V_1,\dots,V_d\}$. We define $e\in \mathbb{R}^d$ to be $e=(1,0,\dots,0)^T$. Obviously the euclidean norm of $e$ is 1. If we insert this into inequality \ref{glg2}, we find that:
\begin{equation*}
V_1=e^TSe \leq \lambda_1\text{.}
\end{equation*}
Therefore $\max\{V_1,\dots,V_n\}$ is a lower bound on $\lambda_1$.


\subsection*{d)}
 We see that the inequality in equation \ref{glg2} becomes tight, if $\lambda_i=\lambda_1$ $\forall i \in \{2,\dots ,n\}$. Therefore lower bound is tight if all eigenvalues of $S$ are equal.



%\begin{thebibliograpy}
%	\bibitem{DHS2000} R. O. Duda, P. E. Hart and D. G. Stork. \emph{Pattern Classification}. 2nd ed. New York, NY, USA: Wiley-Interscience, 2000. ISBN: 0-4710-5669-3
%\end{thebibliograpy}

\end{document}
