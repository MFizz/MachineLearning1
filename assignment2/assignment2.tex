\documentclass[paper=a4,fontsize=10pt,DIV11,BCOR10mm]{scrartcl}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{graphicx}

\usepackage{url}
\usepackage{eurosym}


\usepackage{url}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}




\titlehead{Technische Universität Berlin -- Fachgebiet Maschinelles Lernen\hfill \parbox[t]{2cm}{\includegraphics[width=2cm]{../TU_Logo_kurz_RGB_rot}}}

\begin{document}

\title{Maschinelles Lernen 1 - Assignment 2\\
\small{Technische Universität Berlin}}


\author{\small{Christoph Conrads (315565)}\and \small{Antje Relitz (327289)}  \and \small{Benjamin Pietrowicz (332542)} \and \small{Mitja Richter (324680)} }

\date{WS 2013/2014}

\maketitle



\section{Gauging the Risk}


\begin{flalign*}
	f& = \text{a house will be flooded} & \neg f & = \text{a house won't be flooded}\\
	x & = \text{house stands in a high risk area} & \neg x & = \text{house stands in a low risk area}\\
	P(f) &=0.0005 & P(\neg f)&=0.9995 \\
	P(x) &=0.04 & P(\neg x) & = 0.96 \\
	P(x|f) &= 0.8 & P(\neg x|f)&=0.2 \\
	\alpha_1 & = \text{buying insurance} & \alpha_2 & = \text{not buying insurance}\\
	\lambda(\alpha_1|f)&=1100 \text{\euro} & \lambda(\alpha_1|\neg f)&=1100 \text{\euro}\\
	\lambda(\alpha_2|f)&=100000 \text{\euro} & \lambda(\alpha_2|\neg f)&=0 \text{\euro}\\
\end{flalign*}


\subsection*{a)}

Bayes' formula is
\[ P(A|B) = \frac{P(B|A)P(A)}{P(B)} \]
We want to determine the probability that a house in a high risk area will be flooded, i.\,e., we want to calculate $P(f|x)$:
\[ P(f|x) = \frac{P(x|f)P(f)}{P(x)} = \frac{0.8 \cdot 0.0005}{0.04} = 0.01 \]
There is a 1\% probability that our house will be flooded next year.



\subsection*{b)}

We want to compute how much loss we can expect depending on whether we decide to buy insurance or not. Therefore we compute $R(\alpha_1|x)$ and $R(\alpha_2|x)$:
\begin{align*}
	P(\neg f|x) &= 1 - P(f|x) = 0.99\\
	R(\alpha_1|x) &= \lambda(\alpha_1|f)P(f|x) + \lambda(\alpha_1|\neg f)P(\neg f|x) = 1100\text{\euro} \cdot 0.01 + 1100\text{\euro} \cdot 0.99 = 1100\text{\euro}\\
	R(\alpha_2|x) &= \lambda(\alpha_2|f)P(f|x) + \lambda(\alpha_2|\neg f)P(\neg f|x) = 100000\text{\euro} \cdot 0.01 + 0\text{\euro} \cdot 0.99 = 1000\text{\euro}
\end{align*}
Since $R(\alpha_2|x) < R(\alpha_1|x)$ it would be cheaper not to buy an insurance.



\subsection*{c)}

We feel that the existing calculation matches this group's common sense.





\section{Bounds on the Error}

\subsection*{a)}

\begin{proof}
We have
\[ \min [ P(\omega_1|x), P(\omega_2)|x) ] \stackrel{!}{\leq} 2 P(\omega_1|x) P(\omega_2|x) \]
WLOG let $P(\omega_1 | x) \leq P(\omega_2 | x)$ so that $0 \leq P(\omega_1|x) \leq 0.5$. Then
\[ P(\omega_1|x) \leq 2 P(\omega_1|x) P(\omega_2|x). \]
It holds that $P(\omega_2|x) = 1 - P(\omega_1|x)$ so
\begin{flalign*}
	P(\omega_1|x) &\leq 2 P(\omega_1|x) (1 - P(\omega_1|x)) \\
	&\leq 2 P(\omega_1|x) - 2 P(\omega_1|x)^2 \Leftrightarrow \\
	2 P(\omega_1|x)^2 &\leq P(\omega_1|x)
\end{flalign*}
For $P(\omega_1|x) = 0$, the inequality holds. Otherwise we can divide by that term and get
\[ P(\omega_1|x) \leq 0.5 \]
which is fulfilled because of our assumptions.
\end{proof}



\subsection*{b)}

Let $P(\omega_1|x) = P(\omega_2|x) = 0.5$. Then
\begin{flalign*}
	P(\text{error}|x) = \min [ P(\omega_1|x, P(\omega_2|x) ] &\stackrel{!}{\leq} \alpha P(\omega_1|x) P(\omega_2|x) \\
	0.5 &\leq 0.25 \alpha \Leftrightarrow \\
	\alpha &\geq 2
\end{flalign*}
which violates the assumption $\alpha < 2$.





\section{Gaussian Densities}


\subsection*{a)}

Because $P(\text{error}|x) = P(\omega_2|x)$ and the definition of $P(\text{error}|x)$ we know that $P(\omega_2|x) \leq P(\omega_1|x)$. Using Bayes' rule, it holds that
\[ \frac{P(x|\omega_2)P(\omega_2)}{\cancel{P(x)}} \leq \frac{P(x|\omega_1)P(\omega_1)}{\cancel{P(x)}} \]
We reformulate this to
\begin{equation}
	\frac{P(x|\omega_2)}{P(x|\omega_1)} \leq \frac{P(\omega_1)}{P(\omega_2)}
\end{equation}
and using the definitions we get
\[ \frac{\exp(-(x+\mu)^2 / (2 \sigma^2))}{\exp(-(x-\mu)^2 / (2 \sigma^2))} \leq \frac{P(\omega_1)}{P(\omega_2)} \]
Applying the logarithm gives
\[ \frac{1}{2\sigma^2} \left( (x-\mu)^2 - (x+\mu)^2 \right) = \frac{1}{2\sigma^2} (-4x\mu) \leq \ln{\frac{P(\omega_1)}{P(\omega_2)}} \]
If we want the relationship between $P(\omega_2|x)$ and the conditional error to hold, then
\begin{flalign*}
	\ln P(\omega_2|x) &\leq \ln P(\omega_1|x) + \frac{2x\mu}{\sigma^2} \Leftrightarrow \\
	P(\omega_2|x) &\leq e^{2x\mu/\sigma^2} P(\omega_1|x) \\
\end{flalign*}



\subsection*{b)}

We assume that $P(x|\omega_1) = c e^{-\abs{x-\mu_1}/\sigma^2}, \mu_1 \in \mathbb{R}$ and $P(x|\omega_2) = c e^{-\abs{x-\mu_2}/\sigma^2}, \mu_2 \mathbb{R}$ where $c \in \mathbb{R}$. To simplify the discussion we assume that $\mu_1 \leq \mu_2$. If we insert this into inequality~1, then
\[ \frac{P(x|\omega_2)}{P(x|\omega_1)} = \frac{\exp(-\abs{x - \mu_2}/\sigma^2)}{\exp(-\abs{x - \mu_1}/\sigma^2)} = \exp \left( \frac{\abs{x - \mu_1} - \abs{x - \mu_2}}{\sigma^2} \right) \]
For different $x$ we get
\begin{flalign*}
	\ln \left( \frac{P(x|\omega_2)}{P(x|\omega_1)} \right) = \begin{cases}
		\frac{\mu_1 + \mu_2}{\sigma^2},          & \text{if } x \leq \mu_1 \\
		\frac{\mu_1 + \mu_2 - 2x}{\sigma^2},     & \text{if } \mu_1 < x < \mu_2 \\
		\frac{\mu_1 - \mu_2}{\sigma^2},          & \text{if } x \geq \mu_2 \\
	\end{cases}
\end{flalign*}
So if $x$ is outside of the open interval $(\mu_1, \mu_2)$ the conditional error does not depend on the exact value of $x$. If we assume additionally $\mu_2 = \mu_1$ for the Laplacian distribution, then $P(x|\omega_1) = P(x|\omega_2) > 0$ and using Bayes' rule gives
\[ P(x|\omega_2) P(\omega_2) \leq P(x|\omega_1) P(\omega_1) \Leftrightarrow P(\omega_2) \leq P(\omega_1) \]



%\begin{thebibliograpy}
%	\bibitem{DHS2000} R. O. Duda, P. E. Hart and D. G. Stork. \emph{Pattern Classification}. 2nd ed. New York, NY, USA: Wiley-Interscience, 2000. ISBN: 0-4710-5669-3
%\end{thebibliograpy}

\end{document}
