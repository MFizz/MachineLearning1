\documentclass[paper=a4,fontsize=10pt,DIV11,BCOR10mm]{scrartcl}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{graphicx}

\usepackage{url}
\usepackage{eurosym}


\usepackage{url}




\titlehead{Technische Universität Berlin -- Fachgebiet Maschinelles Lernen\hfill \parbox[t]{2cm}{\includegraphics[width=2cm]{../TU_Logo_kurz_RGB_rot}}}

\begin{document}

\title{Maschinelles Lernen 1 - Assignment 2\\
\small{Technische Universität Berlin}}


\author{\small{Christoph Conrads (315565)}\and \small{Antje Relitz (327289)}  \and \small{Benjamin Pietrowicz (332542)} \and \small{Mitja Richter (324680)} }

\date{WS 2013/2014}

\maketitle



\section{Gauging the Risk}


\begin{flalign*}
	f& = \text{a house will be flooded} & \neg f & = \text{a house won't be flooded}\\
	x & = \text{house stands in a high risk area} & \neg x & = \text{house stands in a low risk area}\\
	P(f) &=0.0005 & P(\neg f)&=0.9995 \\
	P(x) &=0.04 & P(\neg x) & = 0.96 \\
	P(x|f) &= 0.8 & P(\neg x|f)&=0.2 \\
	\alpha_1 & = \text{buying insurance} & \alpha_2 & = \text{not buying insurance}\\
	\lambda(\alpha_1|f)&=1100 \text{\euro} & \lambda(\alpha_1|\neg f)&=1100 \text{\euro}\\
	\lambda(\alpha_2|f)&=100000 \text{\euro} & \lambda(\alpha_1|\neg f)&=0 \text{\euro}\\
\end{flalign*}


\subsection*{a)}

Bayes' formula is
\[ P(A|B) = \frac{P(B|A)P(A)}{P(B)} \]
We want to determine the probability that a house in a high risk area will be flooded, i.\,e., we want to calculate $P(f|x)$:
\[ P(f|x) = \frac{P(x|f)P(f)}{P(x)} = \frac{0.8 \cdot 0.0005}{0.04} = 0.01 \]
There is a 1\% probability that our house will be flooded next year.



\subsection*{b)}

We want to compute how much loss we can expect depending on whether we decide to buy insurance or not. Therefore we compute $R(\alpha_1|x)$ and $R(\alpha_2|x)$:
\begin{align*}
	P(\neg f|x) &= 1 - P(f|x) = 0.99\\
	R(\alpha_1|x) &= \lambda(\alpha_1|f)P(f|x) + \lambda(\alpha_1|\neg f)P(\neg f|x) = 1100\text{\euro} \cdot 0.01 + 1100\text{\euro} \cdot 0.99 = 1100\text{\euro}\\
	R(\alpha_2|x) &= \lambda(\alpha_2|f)P(f|x) + \lambda(\alpha_2|\neg f)P(\neg f|x) = 100000\text{\euro} \cdot 0.01 + 0\text{\euro} \cdot 0.99 = 1000\text{\euro}
\end{align*}
Since $R(\alpha_2|x) < R(\alpha_1|x)$ it would be cheaper not to buy an insurance.



\subsection*{c)}

We feel that the existing calculation matches this group's common sense.





\section{Bounds on the Error}

\subsection*{a)}

\begin{proof}
We have
\[ \min [ P(\omega_1|x), P(\omega_2)|x) ] \stackrel{!}{\leq} 2 P(\omega_1|x) P(\omega_2|x) \]
WLOG let $P(\omega_1 | x) \leq P(\omega_2 | x)$ so that $0 \leq P(\omega_1|x) \leq 0.5$. Then
\[ P(\omega_1|x) \leq 2 P(\omega_1|x) P(\omega_2|x). \]
It holds that $P(\omega_2|x) = 1 - P(\omega_1|x)$ so
\begin{flalign*}
	P(\omega_1|x) &\leq 2 P(\omega_1|x) (1 - P(\omega_1|x)) \\
	&\leq 2 P(\omega_1|x) - 2 P(\omega_1|x)^2 \Leftrightarrow \\
	2 P(\omega_1|x)^2 &\leq P(\omega_1|x)
\end{flalign*}
For $P(\omega_1|x) = 0$, the inequality holds. Otherwise we can divide by that term and get
\[ P(\omega_1|x) \leq 0.5 \]
which is fulfilled because of our assumptions.
\end{proof}



\subsection*{b)}

Let $P(\omega_1|x) = P(\omega_2|x) = 0.5$. Then
\begin{flalign*}
	P(\text{error}|x) = \min [ P(\omega_1|x, P(\omega_2|x) ] &\stackrel{!}{\leq} \alpha P(\omega_1|x) P(\omega_2|x) \\
	0.5 &\leq 0.25 \alpha \Leftrightarrow \\
	\alpha &\geq 2
\end{flalign*}
which violates the assumption $\alpha < 2$.





\section{Gaussian Densities}


\subsection*{a)}

Because $P(\text{error}|x) = P(\omega_2|x)$ and the definition of $P(\text{error}|x)$ we know that $P(\omega_2|x) \leq P(\omega_1|x)$. Using Bayes' rule, it holds that
\[ \frac{P(x|\omega_2)P(\omega_2)}{\cancel{P(x)}} \leq \frac{P(x|\omega_1)P(\omega_1)}{\cancel{P(x)}} \]
We reformulate this to
\[ \frac{P(x|\omega_2)}{P(x|\omega_1)} \leq \frac{P(\omega_1)}{P(\omega_2)} \]
and using the definitions we get
\[ \frac{\exp{(-(x+\mu)^2 / (2 \sigma^2))}}{\exp{(-(x-\mu)^2 / (2 \sigma^2))}} \leq \frac{P(\omega_1)}{P(\omega_2)} \]
Applying the logarithm gives
\[ \frac{1}{2\sigma^2} \left( (x-y)^2 - (x+y)^2 \right) = \frac{1}{2\sigma^2} (-4x\mu) \leq \ln{\frac{P(\omega_1)}{P(\omega_2)}} \]
If we want the relationship between $P(\omega_2|x)$ and the conditional error to hold, then
\begin{flalign*}
	\ln P(\omega_2) &\leq \ln P(\omega_1) + \frac{2x\mu}{\sigma^2} \Leftrightarrow \\
	P(\omega_2) &\leq e^{2x\mu/\sigma^2} P(\omega_1) \\
\end{flalign*}



\subsection*{b)}

We assume two univariate Laplacian densities, i.e.  $p(x|\omega_1) \propto \exp{(-\frac{\lvert x-\mu_1 \rvert}{\sigma})}$ and $p(x|\omega_2) \propto \exp{(-\frac{\lvert x-\mu_2 \rvert}{\sigma})}$.\\

Similar to exercise 3.a) we obtain that 

\begin{equation*}
P(\omega_2 |x) \leq P(\omega_1 |x) \Leftrightarrow \frac{1}{\sigma} \left( \lvert x-\mu_1 \rvert - \lvert x-\mu_2\rvert \right) \leq \ln{\frac{P(\omega_1)}{P(\omega_2)}} \text{.}
\end{equation*}

And again we can reformulate this to
\begin{equation*}
	P(\omega_2) \leq e^{(\frac{\lvert x-\mu_1 \rvert - \lvert x-\mu_2\rvert}{\sigma})} P(\omega_1)\text{.}
\end{equation*}

In the case $\mu_1=\mu_2$, i.e. the Laplacian distributions have the same expected value, this condition simplifies to
\begin{equation*}
	P(\omega_2) \leq P(\omega_1)\text{.}
\end{equation*}



%	\begin{itemize}
%		\item for $x\geq \mu$, condition for $P(\omega_2)$ does not depend on $x$ any more.
%		\item for $x < \mu$, condition for $P(\omega_2)$ does not depend on $\mu$ any more.
%	\end{itemize}



%\begin{thebibliograpy}
%	\bibitem{DHS2000} R. O. Duda, P. E. Hart and D. G. Stork. \emph{Pattern Classification}. 2nd ed. New York, NY, USA: Wiley-Interscience, 2000. ISBN: 0-4710-5669-3
%\end{thebibliograpy}

\end{document}
