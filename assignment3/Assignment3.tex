\documentclass[paper=a4,fontsize=10pt,DIV11,BCOR10mm]{scrartcl}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{graphicx}

\usepackage{url}
\usepackage{eurosym}


\usepackage{url}



\newcommand{\abs}[1]{\left\lvert#1\right\rvert}




\titlehead{Technische Universität Berlin -- Fachgebiet Maschinelles Lernen\hfill \parbox[t]{2cm}{\includegraphics[width=2cm]{../TU_Logo_kurz_RGB_rot}}}

\begin{document}

\title{Maschinelles Lernen 1 - Assignment 3\\
\small{Technische Universität Berlin}}


\author{\small{Christoph Conrads (315565)}\and \small{Antje Relitz (327289)}  \and \small{Benjamin Pietrowicz (332542)} \and \small{Mitja Richter (324680)} }

\date{WS 2013/2014}

\maketitle

\section{Flipping the Coins}

\subsection*{a)}

\[ Pr (D|p) = \prod_{i=1}^7\ Pr(x_i|p) = p^{\# head} * (1-p)^{\# tail} = p^5 * (1-p)^2 = p^5 -2p^6 +p^7 \]



\subsection*{b)}
The maximum likelihood yields:
\[ \nabla Pr (D|p) \stackrel{!}{=} 0 \Leftrightarrow  5p^4 - 12 p^5 + 7p^6 = 0 \Leftrightarrow p^4(5-12p+7p^2) = 0 \Leftrightarrow p^4 = 0 \lor 5-12p+7p^2 = 0 \]\\
From our given sequence of events we know that $p\in ]0,1[$\\ 
\[\Rightarrow 5-12p+ 7p^2 = 0 \Leftrightarrow p^2 - \frac{12}{7}p + \frac{5}{7} = 0 \Leftrightarrow p_{1,2} = \frac{6}{7} \pm \sqrt{\frac{36}{49} - \frac{35}{49}} \Leftrightarrow p_{1,2} = \frac{6}{7} \pm \sqrt{\frac{1}{49}} \Leftrightarrow\]
\[ p_{1,2} = \frac{6}{7} \pm \frac{1}{7} \Leftrightarrow p_1 = \frac{5}{7} \land p_2 = \frac{7}{7} = 1\]
Again we know that $p<1$
\[ \Rightarrow p = \frac{5}{7} \]
\\We are now looking for the probability of the given sequence $D_1 = (x_8,x_9) = (head,head)$:
\[Pr(D_1|\frac{5}{7}) = \prod_{i=8}^9\ Pr(x_i|\frac{5}{7}) =Pr(\{x_8=head\})*Pr(\{x_9 = head\}) = \frac{5}{7} * \frac{5}{7} = \frac{25}{49} \]
The probability that the next two tosses are "head" with the given unfair coin is
$ p = \frac{25}{49}$.

\section{Biased Boundaries}
\subsection*{b)}
	Our Maximum Likelihood function for $P(D_1|\mu_1)$:
	\[
	l(\mu_1) = \ln P(D_1|\mu_1) = \sum_{i=1}^n \ln P(x_i|\mu_1)
	\]
	Under the Gaussian generative assumtpion we get:
	\[
		P(x_i|\mu_1) = \frac{1}{\sqrt{2\pi}\sigma}\exp^{-\frac{1}{2\sigma^2}(x_i-\mu_1)^2}\\
	\]
	Applying the logarhitmus for convenience:
	\[
		\ln P(x_i|\mu_1) =-\frac{1}{2} \ln 2\pi \sigma -\frac{1}{2\sigma^2}(x_i-\mu_1)^2\\
	\]
	Computing the derivate:
	\[	
		\frac{d \ln P(x_i|\mu_1)}{d \mu_1} = \frac{1}{\sigma}(x_i- \mu_1)\\
	\]
	For the dataset $D$ we get:
	\[
		\sum_{i=1}^n \frac{1}{\sigma}(x_i- \hat{\mu}_1) \stackrel{!}{=} 0\\
	\rightarrow \hat{\mu}_1=\frac{1}{n} \sum_{i=1}^n x_i \qed
	\]
\section{Feature Expansion}
	\subsection*{a)} There are several general problems with using non-linear feature mapping into higher dimensions in this situation:
	\begin{itemize}
	\item The first problem is the large number of unknown parameters required to learn the gaussian ML-model. With increasing dimension $d$ of the input vector, the estimate of the mean grows linear to a $d$-dimension vector, but the covariance matrix $\Sigma$ ($d \times d)$ is growing quadratically.
	\item The next problem would be the additional computational load of computing $\phi(x)$, which is especially grave with a large dataset $D$.
	\item Furthermore there is the danger of overfitting, since the linear (or quadratic) discriminant is segmenting the training data non-linearly in lower input space. With a noisy dataset this could lead to false classifications.
	\end{itemize}
	On the other side this methods produces a linear (or quadratic) discriminant, which makes classification really fast.
	
	\subsection*{b)}
	With a small dataset, the bias of ML can potentially distort results, since ML is only asymptotically unbiased. On the other hand a small dataset reduces the additional load of computing $\phi(x)$
	
	\subsection*{c)} If we are getting a linear discriminant with the method described above, changes in the priors should only result in shifts of the boundary hyperplane away from the more likely prior, which should not affect overall complexity or accuracy.
	






%\begin{thebibliograpy}
%	\bibitem{DHS2000} R. O. Duda, P. E. Hart and D. G. Stork. \emph{Pattern Classification}. 2nd ed. New York, NY, USA: Wiley-Interscience, 2000. ISBN: 0-4710-5669-3
%\end{thebibliograpy}

\end{document}
